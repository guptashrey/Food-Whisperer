{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3cafef-b3fc-4294-81ca-3b6acead188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c1bc72-2f43-4f0a-aa80-56d7ac8a2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93145e0d-3a45-4f1f-b65a-ad43a03c393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 21:31:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8873e704829040a1b23a8216f2743605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 21:31:57 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-04-07 21:31:57 INFO: Using device: cuda\n",
      "2023-04-07 21:31:57 INFO: Loading: tokenize\n",
      "2023-04-07 21:32:01 INFO: Loading: pos\n",
      "2023-04-07 21:32:01 INFO: Loading: lemma\n",
      "2023-04-07 21:32:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma ,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c53590fa-bc98-413e-836f-45b921319b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ad1c84-600c-46dd-b6ac-e858b1c72499",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_df = pd.read_csv(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99da2a40-68fd-4a07-974b-6b8de7c5d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_list = list(ingredient_df.ingredient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5ab3fe-f507-4fcb-9c7a-8ce3d605a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_res=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29935c4c-7bef-45b6-96ca-3c285020353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14901/14901 [05:33<00:00, 44.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(ingredient_list):\n",
    "    stanza_res.append(nlp(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f422f6-0bb0-4720-aa78-870f8f5a3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781300cf-cc55-40ad-bc9e-b954a7648d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14901/14901 [00:00<00:00, 295556.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6860\n",
      "6861\n",
      "6862\n",
      "9603\n",
      "10054\n",
      "10055\n",
      "11795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for res in tqdm(stanza_res):\n",
    "    noun_list = []\n",
    "    if len(res.sentences) > 1:\n",
    "        print(i)\n",
    "    \n",
    "    else:\n",
    "        for sentence in res.sentences:\n",
    "            noun_list = []\n",
    "            len_word = len(sentence.words)\n",
    "            j = 0\n",
    "            for word in sentence.words:\n",
    "                if (len_word - 1) == j:\n",
    "                    if word.upos == \"NOUN\":\n",
    "                        noun_list.append(word.lemma)\n",
    "                    else:\n",
    "                        noun_list = []\n",
    "                else:\n",
    "                    if word.upos == \"NOUN\":\n",
    "                        noun_list.append(word.text)\n",
    "                    else:\n",
    "                        noun_list = []\n",
    "                j = j + 1\n",
    "                    \n",
    "    final_words.append(\" \".join(noun_list))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbb1e23c-294b-403b-948b-72510ce90528",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([ingredient_df, pd.DataFrame(final_words)], axis=1).to_csv(\"Foo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea9fe420-deb1-48d8-b873-d7166090b913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1% fat buttermilk'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredient_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b454a6a-5d75-4ec8-8bf4-fd0d4623a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(ingredient_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffcdca94-77e3-4ef1-834a-a118b42c51e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sentence' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sentence' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "doc.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ce554f5-86fd-4ccf-911c-d4d7e6832f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buttermilk\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.upos == \"NOUN\":\n",
    "            print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e33a7d8-3fe8-4491-b86a-e7a35afb3843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0703042-9863-4fc0-9864-efd96629a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adjectives_with_nouns(review_text, nlp_stanza):\n",
    "    \"\"\"\n",
    "    Extract the adjectives and the nouns they are describing\n",
    "\n",
    "    Args:\n",
    "        review_text (str): review text\n",
    "    \n",
    "    Returns:\n",
    "        adj_noun_pairs (list): list of adjective-noun pairs\n",
    "    \"\"\"\n",
    "\n",
    "    ## extract all the adjectives and the nouns they are describing\n",
    "    doc = nlp_stanza(review_text)\n",
    "    adj_noun_pairs = []\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        \n",
    "        for word in sentence.words:\n",
    "            if word.upos == \"NOUN\":\n",
    "                new_list.append(word)\n",
    "\n",
    "    return adj_noun_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d64a4c-92da-492c-80d9-0a15987e0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Barack Obama was born in Hawaii.')\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc81d74-ea90-4c6b-890b-28f1cdd64446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the Universal Sentence Encoder's TF Hub module\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ae1d6f-87dd-4ba7-bf9d-b86cfbecdbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:50:15.854452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:16.139046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:16.139456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:16.140675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 19:50:16.142324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:16.142705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:16.143021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:28.714173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:28.740275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:28.741315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 19:50:28.742136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46704 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:04:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder-large/5 loaded\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6d7b1d5-beb7-4e83-ac79-1bacc9ccafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d916a89-f1ba-470f-9f19-5733c065dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_plot(messages_):\n",
    "    message_embeddings_ = embed(messages_)\n",
    "    corr = np.inner(message_embeddings_, message_embeddings_)\n",
    "    return corr, messages_, message_embeddings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b51f1-a21b-443f-8df6-e26870372a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_540",
   "language": "python",
   "name": "cv_540"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
